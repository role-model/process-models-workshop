# ML model selection with MESS simulations

## Key questions

1. How do I perform a inference using MESS simulations in R?

## Lesson objectives

After this lesson, learners should be able to...

1. Use RoLE simulations and RandomForest ML (w/ tidymodels) to perform inference
1. Interpret results in terms of story
1. Brainstorm applications to real data

## Planned exercises


* Lecture: How does ML work with MESS simulations?
* Run MESS model simulations
* ML assembly model classification
* ML parameter estimation
* Posterior predictive simulations (if time)
* Free time to experiment with other example datasets

### Lecture: How does ML work with MESS simulations?
Lecture: [How does machine learning work with MESS simulations?](https://docs.google.com/presentation/d/12fVO8Jzxvdm5nxcvITtpn3re8VJm4j1J96Y3zmBh6ZY/edit?usp=sharing)

#### Download the pre-baked simulations
Since it can take quite some time to run a number of simulations sufficient for
model selection and parameter estimation we will use a suite of pre-baked
simulations generated ahead of time. We want to practice good organizational
habits for our simulations and .R scripts, so as a challenge, before downloading
the simulations file, **make a new directory in your home directory called
"MESS-inference" and change directory into it.* 

::: {.callout-note collapse="true"}
## Solution: Make a 'MESS-inference' directory in your home directory

```default
## Make sure you are in your home directory
cd ~
## Make the 'MESS-inference' directory
mkdir MESS-inference
## change into the new directory
cd MESS-inference
## print working directory to verify you are where you should be
pwd
```
```default
/home/rstudio/MESS-inference
```

:::

Once you are in the `/home/rstudio/MESS-inference` directory, you may fetch
the simulations from the workshop site using `wget`:

```default
wget https://github.com/role-model/process-models-workshop/raw/main/data/MESS-SIMOUT.csv.gz
gunzip MESS-SIMOUT.csv.gz
wc -l MESS-SIMOUT.txt

```
    MESS-SIMOUT.csv.gz   100%[====================>]  35.24M  1.12MB/s    in 32s
    2023-06-15 14:31:54 (1.12 MB/s) - ‘MESS-SIMOUT.csv.gz’ saved [36954693/36954693]
    3000 SIMOUT.txt

:::{.callout-note}
The `wc` command counts the number of lines if you pass it the `-l` flag.
You can see this series of 3000 simulations is about 35MB gzipped.
:::

### ML assembly model classification
The first step is now to assess the model of community assembly that best
fits the data. The three models are `neutral`, in which all individuals are
ecologically equivalent; `competition`, in which species have traits, and
differential survival probability is weighted by distance of traits from
the trait mean in the local community (closer to the trait mean == higher
probability of death); and `filtering`, in which there is an environmental
optimum, and proximity of trait values to this optimum is positively
correlated with survival probability.

Basically we want to know, are individuals in the local community ecologically
equivalent, and if not are they interacting more with each other or more
with the local environment.

Looking in the "Files" tab, double click on the new "MESS-inference" folder and
you should see your new "MESS-SIMOUT.csv.gz" file. Now you can create a "New
Blank File" in this directory by clicking on the small green plus.

![Make a new blank file for the MESS inference R code](images/MESS-Classification-NewBlankFile.png)

Now we will install a couple necessary packages: `randomForest` and `caret`, two
machine learning packages for R.

```R
install.packages("randomForest")
install.packages(pkgs = "caret", dependencies = c("Depends", "Imports"))
library(randomForest)
library(caret)
```

```R
simdata = MESS$load_local_sims("MESS-SIMOUT.csv")[[1]]

simdata$assembly_model <- as.factor(simdata$assembly_model)
table(simdata$assembly_model)
```
```default
competition     filtering   neutral
       1000          1000       999
```

#### Train the ML classifier

```R
## 70/30 test/train split
tmp <- sample(2, nrow(simdata), replace = TRUE, prob = c(0.7, 0.3))
train <- simdata[tmp==1,]
test <- simdata[tmp==2,]
```

```R
## Experiment with results for different axes of data!
rf <- randomForest(assembly_model ~ local_S + pi_h1 + abund_h1 + trait_h1, data=train, proximity=TRUE)
print(rf)
```
```default
Call:
 randomForest(formula = assembly_model ~ local_S + pi_h1 + abund_h1 +      trait_h1, data = train, proximity = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 19.41%
Confusion matrix:
            competition filtering neutral class.error
competition         538        33     124   0.2258993
filtering            24       571      89   0.1652047
neutral              61        71     560   0.1907514
```

#### Plot confusion matrix
```R
test_predictions <- predict(rf, test)
cm <- confusionMatrix(test_predictions, test$assembly_model)
```
```default
Confusion Matrix and Statistics

             Reference
Prediction    competition filtering neutral
  competition         233        13      32
  filtering            23       270      28
  neutral              49        33     247

Overall Statistics
                                         
               Accuracy : 0.8082         
                 95% CI : (0.7814, 0.833)
    No Information Rate : 0.3405         
    P-Value [Acc > NIR] : < 2e-16        
                                         
                  Kappa : 0.7122         
                                         
 Mcnemar's Test P-Value : 0.08011        

Statistics by Class:

                     Class: competition Class: filtering Class: neutral
Sensitivity                      0.7639           0.8544         0.8046
Specificity                      0.9278           0.9167         0.8680
Pos Pred Value                   0.8381           0.8411         0.7508
Neg Pred Value                   0.8892           0.9242         0.8998
Prevalence                       0.3287           0.3405         0.3308
Detection Rate                   0.2511           0.2909         0.2662
Detection Prevalence             0.2996           0.3459         0.3545
Balanced Accuracy                0.8459           0.8855         0.8363
```

```R
heatmap(cm$table, Rowv=NA, Colv=NA, margins=c(12, 12), xlab="Prediction", ylab="Reference")
```
![Visualization of the confusion matrix](images/MESS-Classification-ConfusionMatrix.png)

```R
varImpPlot(rf,
           sort = T,
           n.var = 4,
           main = "Variable Importance")
```

![Variable importance in RF classification](images/MESS-Classification-VariableImportance.png)

```R
MDSplot(rf, train$assembly_model)
```

![MDSplot depicting the clustering of simulations by assembly model](images/MESS-Classification-MDSplot.png)

### Perform classification of empirical data and view results
```R
p_emp <- predict(rf, test[1, ], type="prob")
p_emp
```
```default
  competition filtering neutral
1       0.016      0.04   0.944
```


### ML parameter estimation
Now that we have identified the neutral model as the most probable, we can
estimate parameters of the emipirical data given this model. Essentially,
we are asking the question "What are the parameters of the model that
generate summary statistics most similar to those of the empirical data?"

#### Train the ML regressor
#### Plot cross-validation results
#### Perform regression to predict dispersal_prob of empirical data and view results

## Key points
* Key
* point
* one
